\chapter{Introduction}

One of the main challenges in computer graphics is physically based rendering, the synthetic creation of images that are indistinguishable from the perception of the real world based on a geometric description of the scene, materials and light sources. There are several algorithms that try to solve this problem, although none of them is yet robust enough to handle every possible situation.

One of the first solutions, \gls{pt} proposed by \cite{Kajiya}, aims to solve this problem by tracing ligh transport path starting from the camera until it hits a light source.

One improvement upon this algorithm was \gls{bdpt}, developed independently by \cite{Lafortune} and \cite{Veach}. Although with different mathematical background, the goal is to sample more light transport paths by connecting sub-paths generated from the camera and the light source. This allows for a much more efficient rendering of effects like caustics, although
effects like reflected caustics are still too difficult for a bidirectional path tracer to handle robustly.

In an effort to improve the efficiency and robustness of light transport algorithms, \cite{Veach} proposed the adaptation of the metropolis sampling algorithm to the light transport problem. The metropolis algorithm consists in starting from any point in the function domain, we apply mutations to this point with a carefully chosen acceptance probability, and the sampling pattern will be proportional to the value of the function. In the case of rendering, the image function is the estimated incident radiance value for each pixel and the integration domain is the set of all light transport paths.

One completely different approach developed by \cite{Jensen} was instead of trying to find paths from the light source to the camera to just trace a packets of photons throughout the scene and store them in an acceleration structure. In a second pass, the rays would start from the camera and consult the photon map in the vicinity of the intersections and calculate the expected radiance through a density estimation. Unlike all the previously presented methods, photon mapping introduces bias, that is, it may not converge to the correct result of the rendering equation. Although, it is consistent, and by diminishing the search radius on the photon map and increasing the number of traced photons, the bias reduces to zero in the limit \citep{Hachisuka}.

Most recently, an attempt to combine these two approaches was proposed by \cite{Georgiev}. In his algorithm, \gls{vcm}, photon mapping and bidirectional path tracing are combined, taking advantage of each of the algorithms strong points: the high convergence rate from bidirectional path tracing and the better handling of caustics from photon mapping. These two algorithms are combined by reducing photon mapping to a path sampling technique in the path integration space formulation and combines it with bidirectional path tracing using multiple importance sampling.

Despite the different aproaches usen in different techniques, what they all have in common is the need for computaional power, since all of these algorithms are based on ray tracing, a computationally expensive operation.

Although most computers nowadays contain both a \gls{cpu} and a \gls{gpu}, most commonly seen implementations only use one of there processors, and so wasting usefull computational power. Developing for this kind of hybrid platforms is not easy though, as it involves maintaining different implementations for each processor as well as dealing with separate memory addressing spaces. In order to solve these issues there is the framework DICE, developed in the University of Texas at Austin. This framework allows an easy workload distribution as well as a memory management system. Mapping light transport algorithms to these heterogeneous systems may lead to performance improvements although no test to evaluate the suitability of these algorithms has been conducted yet. 

With all this, the main goals for this project are to eavluate the relative performance and robustness of the light transport algorithms described previously, evaluate their suitability for heterogeneous systems and evaluate the DICE framework in terms of performance, usability and the possibilities provided.